import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset

# --- 1. Load and Prepare Your Data ---
print("Loading dataset...")
df = pd.read_csv("dataset.csv")

# We need at least a few examples to run. Let's make a small default if the file is too small.
if len(df) < 10:
    print("Warning: Dataset is very small. Using dummy data for training.")
    dummy_data = {
        'text': [
            "This is a great example of human writing.",
            "I went to the store and bought some milk.",
            "The quick brown fox jumps over the lazy dog.",
            "Tomorrow's weather looks sunny.",
            "Can you send me the report from last week?",
            "In the realm of digital innovation, synergy is key.",
            "Harnessing the power of the cloud allows for scalable solutions.",
            "Let's delve into the multifaceted aspects of this paradigm.",
            "The new AI model showcases unprecedented capabilities.",
            "This text was generated by a large language model."
        ],
        'label': [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]
    }
    df = pd.DataFrame(dummy_data)


# Split the dataset into training and testing sets
train_texts, test_texts, train_labels, test_labels = train_test_split(
    df['text'].tolist(), df['label'].tolist(), test_size=0.2, random_state=42
)

# Convert to Hugging Face's Dataset format
train_dataset = Dataset.from_dict({'text': train_texts, 'label': train_labels})
test_dataset = Dataset.from_dict({'text': test_texts, 'label': test_labels})

# --- 2. Initialize Tokenizer and Model ---
model_name = "distilbert-base-uncased"
print(f"Loading tokenizer and model for {model_name}...")
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

# --- 3. Tokenize the Data ---
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

print("Tokenizing datasets...")
train_dataset = train_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

# --- 4. Set Up Simplified Trainer ---
print("Setting up simplified training arguments...")
# This is a simplified configuration that avoids the problematic argument.
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=1,              # Set to 1 for a quick test run
    per_device_train_batch_size=8,
    logging_steps=10,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    # We remove the eval_dataset for this simplified run
)

# --- 5. Train the Model ---
print("Starting training...")
trainer.train()
print("Training complete!")

# --- 6. Save the Model ---
final_model_path = "./my_custom_ai_detector"
trainer.save_model(final_model_path)
tokenizer.save_pretrained(final_model_path)
print(f"Model successfully saved to {final_model_path}")